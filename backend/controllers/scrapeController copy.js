// controllers/scrapeController.js
const cheerio = require('cheerio');
const puppeteer = require('puppeteer');
const { Tokenizer } = require('../utils/tokenizer');
const VectorDB = require('../utils/vectorDB');
const { cleanHTML } = require('../utils/htmlCleaner');

const vectorDB = new VectorDB();
const tokenizer = new Tokenizer();

exports.handleScrape = async (req, res) => {
  const { url, query } = req.body;
  if (!url || !query) {
    return res.status(400).json({ error: "URL and query are required." });
  }
  
  try {
    // Enhanced browser configuration
    const browser = await puppeteer.launch({ 
      headless: "new",
      args: ['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage']
    });
    
    const page = await browser.newPage();
    
    // Set viewport and user agent for better compatibility
    await page.setViewport({ width: 1280, height: 800 });
    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36');
    
    // Better page loading strategy
    await page.goto(url, { 
      waitUntil: "networkidle2",
      timeout: 60000 // Increase timeout to 60 seconds
    });
    
    // Wait for content to be properly loaded
    await page.waitForSelector('body', { timeout: 10000 });
    
    // Get the full HTML content
    const html = await page.content();
    
    // Try a different approach - directly extract text from the page
    const bodyText = await page.evaluate(() => document.body.innerText);
    
    await browser.close();

    // Clean the HTML
    const cleanedHTML = cleanHTML(html);
    
    // Use cheerio to parse both the cleaned HTML and original HTML
    const $ = cheerio.load(cleanedHTML);
    
    // Collect all text content for proper tokenization
    let allText = "";
    
    // First, try direct body text if it exists
    if (bodyText && bodyText.length > 0) {
      allText = bodyText;
    } else {
      // Fallback to extracted text from HTML if direct body text isn't available
      // Extract text from all paragraph elements
      $('p').each((_, el) => {
        const text = $(el).text().trim();
        if (text.length > 20) {
          allText += text + "\n\n";
        }
      });
      
      // Extract all headings
      $('h1, h2, h3, h4, h5, h6').each((_, el) => {
        const text = $(el).text().trim();
        if (text.length > 10) {
          allText += text + "\n\n";
        }
      });
      
      // Look for div elements with substantial text
      $('div').each((_, el) => {
        const text = $(el).text().trim();
        if (text.length > 50 && $(el).children().length < 5) {
          allText += text + "\n\n";
        }
      });
      
      // Get list items
      $('li').each((_, el) => {
        const text = $(el).text().trim();
        if (text.length > 15) {
          allText += text + "\n\n";
        }
      });
      
    }

    if (allText.length === 0) {
      return res.status(400).json({ 
        error: "No valid content found on the page.",
        htmlLength: html.length,
        cleanedHtmlLength: cleanedHTML.length,
        bodyTextLength: bodyText ? bodyText.length : 0
      });
    }

    // Properly use the tokenizer to chunk the text
    // Set a reasonable max token count for chunks
    const maxTokens = 500;
    const chunks = tokenizer.chunkText(allText, maxTokens);
    
    

    if (chunks.length === 0) {
      return res.status(400).json({ error: "No valid chunks generated by tokenizer." });
    }

    // Index chunks and perform search
    try {
      await vectorDB.indexChunks(chunks);
      
      // Also use tokenizer for the query
      const queryTokens = tokenizer.tokenize(query);
      
      const results = await vectorDB.semanticSearch(query, 10);
      
      
      if (results.length === 0) {
        // If no results, return some chunks as fallback
        return res.json({
          results: chunks.slice(0, 5).map(chunk => ({
            text: chunk.text,
            score: 0.1,
            tokenCount: chunk.tokenCount,
            matches: [{
              text: chunk.text,
              path: "/"
            }]
          })),
          totalChunks: chunks.length,
          totalResults: 5,
          message: "No exact matches found for your query. Showing sample content from the page."
        });
      }

      // Create result objects
      const enrichedResults = results.map(result => {
        return {
          text: result.text,
          score: result.score || 0.1,
          tokenCount: result.tokenCount || tokenizer.countTokens(result.text),
          matches: [{
            text: result.text,
            path: "/"
          }]
        };
      });

      // Add token counts to the response
      const queryTokenCount = tokenizer.countTokens(query);
      
      res.json({ 
        results: enrichedResults,
        totalChunks: chunks.length,
        totalResults: enrichedResults.length,
        queryTokenCount,
        message: `Found ${enrichedResults.length} results related to "${query}" (${queryTokenCount} tokens)`
      });
    } catch (error) {
      // Even if search fails, return some content
      res.json({
        results: chunks.slice(0, 5).map(chunk => ({
          text: chunk.text,
          score: 0.1,
          tokenCount: chunk.tokenCount,
          matches: [{
            text: chunk.text,
            path: "/"
          }]
        })),
        totalChunks: chunks.length,
        totalResults: 5,
        message: "Error in semantic search. Showing sample content from the page.",
        error: error.message
      });
    }

  } catch (error) {
    res.status(500).json({ 
      error: "Failed to process the URL",
      details: error.message,
      stack: error.stack
    });
  }
};